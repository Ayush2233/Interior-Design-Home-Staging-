{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad01267-2aa1-4557-9fa5-676bff5ac6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import copy\n",
    "import argparse\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ac103c-23ff-4028-bb7b-fe525c69a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "num_steps = 700\n",
    "style_weight = 1000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db302295-4d05-453c-ac44-fa94a15fc3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target, ):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # we 'detach' the target content from the tree used\n",
    "        # to dynamically compute the gradient: this is a stated value,\n",
    "        # not a variable. Otherwise the forward method of the criterion\n",
    "        # will throw an error.\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af34f735-1cc5-468a-9b4e-65f000d356f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()  # a=batch size(=1)\n",
    "    # b=number of feature maps\n",
    "    # (c,d)=dimensions of a f. map (N=c*d)\n",
    "\n",
    "    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
    "\n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "    # we 'normalize' the values of the gram matrix\n",
    "    # by dividing by the number of element in each feature maps.\n",
    "    return G.div(a * b * c * d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec3a854-6bfa-45b1-9ec8-82fce983c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c0f3070-8467-42d1-beba-3dfc39442580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        # .view the mean and std to make them [C x 1 x 1] so that they can\n",
    "        # directly work with image Tensor of shape [B x C x H x W].\n",
    "        # B is batch size. C is number of channels. H is height and W is width.\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # normalize img\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d035f34-d9c8-466b-bcdb-32521adb330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                               style_img, content_img,\n",
    "                               content_layers,\n",
    "                               style_layers):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "\n",
    "    # normalization module\n",
    "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "\n",
    "    # just in order to have an iterable access to or list of content/syle\n",
    "    # losses\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n",
    "    # to put in modules that are supposed to be activated sequentially\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    i = 0  # increment every time we see a conv\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            # The in-place version doesn't play very nicely with the ContentLoss\n",
    "            # and StyleLoss we insert below. So we replace with out-of-place\n",
    "            # ones here.\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            # add content loss:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            # add style loss:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # now we trim off the layers after the last content and style losses\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "\n",
    "    model = model[:(i + 1)]\n",
    "\n",
    "    return model, style_losses, content_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d011be-75fb-4538-8bce-dceae7dd0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_optimizer(input_img):\n",
    "    # this line to show that input is a parameter that requires a gradient\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d97416-dbf8-451f-ba81-be68c73e655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(image_name, target_size):\n",
    "    image = Image.open(image_name)\n",
    "    # Resize the image to the target size\n",
    "    image = transforms.Resize(target_size)(image)\n",
    "    # Convert to tensor and add a batch dimension\n",
    "    image = transforms.ToTensor()(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c091786-7946-4ddc-b9a5-04b4e94ddaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_loader(folder_path, target_size):\n",
    "    # Get the list of image files in the folder\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    \n",
    "    # Create a DataLoader for the images in the folder\n",
    "    dataset_loader = []\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        image = image_loader(image_path, target_size)\n",
    "        dataset_loader.append(image)\n",
    "    \n",
    "    return dataset_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f4a7d48-7992-4b1f-a9f3-bdc5c916fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                       content_img, style_img, input_img, num_steps,\n",
    "                       style_weight, content_weight=1):\n",
    "    \"\"\"Run the style transfer.\"\"\"\n",
    "    print('Building the style transfer model..')\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "                                                                     normalization_mean, normalization_std, style_img,\n",
    "                                                                     content_img,content_layers_default, style_layers_default)\n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "\n",
    "    print('Optimizing..')\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            # correct the values of updated input image\n",
    "            input_img.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "\n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "\n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(\"run {}:\".format(run))\n",
    "                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
    "                    style_score.item(), content_score.item()))\n",
    "                print()\n",
    "\n",
    "            return style_score + content_score\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # a last correction...\n",
    "    input_img.data.clamp_(0, 1)\n",
    "\n",
    "    return input_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7057fee3-136c-4bb2-a8c9-3ea8beb5152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer_on_dataset(cnn, normalization_mean, normalization_std,\n",
    "                                  content_dataset, style_dataset, num_steps,\n",
    "                                  style_weight, content_weight=1, output_path=\"C:/Users/AYUSH/Deep Learning Project/outputimages/\"):\n",
    "    for i, content_img in enumerate(content_dataset):\n",
    "        for j, style_img in enumerate(style_dataset):\n",
    "            # Initialize the input image as a copy of the content image\n",
    "            input_img = content_img.clone()\n",
    "\n",
    "            # Run style transfer\n",
    "            output = run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                                        content_img, style_img, input_img, num_steps, style_weight, content_weight)\n",
    "\n",
    "            # Save the output image\n",
    "            torchvision.utils.save_image(output, os.path.join(output_path, f\"output_{i}_{j}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8779f4fa-1bb8-4e87-baed-4985af6bb8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def st(style_img, content_img):\n",
    "    ######################################################################\n",
    "    # Importing the Model\n",
    "    # -------------------\n",
    "    cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "    ######################################################################\n",
    "    # Additionally, VGG networks are trained on images with each channel\n",
    "    # normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
    "    # We will use them to normalize the image before sending it into the network.\n",
    "    #\n",
    "\n",
    "    cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "    cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # Next, we select the input image. You can use a copy of the content image\n",
    "    # or white noise.\n",
    "    #\n",
    "    input_img = content_img.clone()\n",
    "\n",
    "    output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                       content_img, style_img, input_img, num_steps, style_weight)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c15f1f0-c9f8-476c-8c63-6c19402cbff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the style transfer model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AYUSH\\AppData\\Local\\Temp\\ipykernel_13084\\4145313249.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
      "C:\\Users\\AYUSH\\AppData\\Local\\Temp\\ipykernel_13084\\4145313249.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std = torch.tensor(std).view(-1, 1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 19.993582 Content Loss: 7.240349\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 6.942190 Content Loss: 5.707569\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 4.999226 Content Loss: 4.757929\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 4.042702 Content Loss: 4.318666\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 3.250791 Content Loss: 4.145459\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 2.546430 Content Loss: 4.071249\n",
      "\n",
      "run [350]:\n",
      "Style Loss : 1.981914 Content Loss: 4.024672\n",
      "\n",
      "run [400]:\n",
      "Style Loss : 1.569494 Content Loss: 3.972436\n",
      "\n",
      "run [450]:\n",
      "Style Loss : 1.321036 Content Loss: 3.904739\n",
      "\n",
      "run [500]:\n",
      "Style Loss : 1.163311 Content Loss: 3.832359\n",
      "\n",
      "run [550]:\n",
      "Style Loss : 1.070525 Content Loss: 3.767200\n",
      "\n",
      "run [600]:\n",
      "Style Loss : 1.007565 Content Loss: 3.712166\n",
      "\n",
      "run [650]:\n",
      "Style Loss : 0.966649 Content Loss: 3.664161\n",
      "\n",
      "run [700]:\n",
      "Style Loss : 0.937533 Content Loss: 3.623591\n",
      "\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 64.147926 Content Loss: 9.888753\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 26.318649 Content Loss: 10.334475\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 16.934347 Content Loss: 9.517735\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 11.984607 Content Loss: 8.937594\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 8.010014 Content Loss: 8.630281\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 5.177423 Content Loss: 8.388487\n",
      "\n",
      "run [350]:\n",
      "Style Loss : 3.863207 Content Loss: 8.081069\n",
      "\n",
      "run [400]:\n",
      "Style Loss : 3.275660 Content Loss: 7.805459\n",
      "\n",
      "run [450]:\n",
      "Style Loss : 2.957679 Content Loss: 7.592699\n",
      "\n",
      "run [500]:\n",
      "Style Loss : 2.724452 Content Loss: 7.449764\n",
      "\n",
      "run [550]:\n",
      "Style Loss : 2.546527 Content Loss: 7.339866\n",
      "\n",
      "run [600]:\n",
      "Style Loss : 2.405651 Content Loss: 7.259934\n",
      "\n",
      "run [650]:\n",
      "Style Loss : 2.286057 Content Loss: 7.201326\n",
      "\n",
      "run [700]:\n",
      "Style Loss : 2.194756 Content Loss: 7.154090\n",
      "\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 207.540909 Content Loss: 12.139892\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 53.017994 Content Loss: 12.465816\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 27.702059 Content Loss: 11.948578\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 15.006055 Content Loss: 11.478571\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 6.941768 Content Loss: 10.947285\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 4.206762 Content Loss: 10.042611\n",
      "\n",
      "run [350]:\n",
      "Style Loss : 3.390828 Content Loss: 9.244354\n",
      "\n",
      "run [400]:\n",
      "Style Loss : 2.982345 Content Loss: 8.729371\n",
      "\n",
      "run [450]:\n",
      "Style Loss : 2.702904 Content Loss: 8.372568\n",
      "\n",
      "run [500]:\n",
      "Style Loss : 2.468515 Content Loss: 8.105456\n",
      "\n",
      "run [550]:\n",
      "Style Loss : 2.295780 Content Loss: 7.920633\n",
      "\n",
      "run [600]:\n",
      "Style Loss : 2.168652 Content Loss: 7.782456\n",
      "\n",
      "run [650]:\n",
      "Style Loss : 2.143190 Content Loss: 7.666008\n",
      "\n",
      "run [700]:\n",
      "Style Loss : 1.993292 Content Loss: 7.570053\n",
      "\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 17.718134 Content Loss: 7.174314\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 6.227882 Content Loss: 6.657259\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 4.420231 Content Loss: 5.825499\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 3.528038 Content Loss: 5.430614\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 2.909844 Content Loss: 5.196240\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 2.474914 Content Loss: 5.032952\n",
      "\n",
      "run [350]:\n",
      "Style Loss : 2.115877 Content Loss: 4.925723\n",
      "\n",
      "run [400]:\n",
      "Style Loss : 1.819999 Content Loss: 4.837841\n",
      "\n",
      "run [450]:\n",
      "Style Loss : 1.590820 Content Loss: 4.764257\n",
      "\n",
      "run [500]:\n",
      "Style Loss : 1.443209 Content Loss: 4.691492\n",
      "\n",
      "run [550]:\n",
      "Style Loss : 1.342520 Content Loss: 4.624904\n",
      "\n",
      "run [600]:\n",
      "Style Loss : 1.273804 Content Loss: 4.570619\n",
      "\n",
      "run [650]:\n",
      "Style Loss : 1.231926 Content Loss: 4.524514\n",
      "\n",
      "run [700]:\n",
      "Style Loss : 1.188816 Content Loss: 4.478349\n",
      "\n",
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 46.214760 Content Loss: 6.341143\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 24.008186 Content Loss: 7.208593\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 12.976194 Content Loss: 7.073792\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 7.852342 Content Loss: 6.446591\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 5.166855 Content Loss: 5.896158\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 3.668471 Content Loss: 5.474687\n",
      "\n",
      "run [350]:\n",
      "Style Loss : 2.798391 Content Loss: 5.154473\n",
      "\n",
      "run [400]:\n",
      "Style Loss : 2.252365 Content Loss: 4.951884\n",
      "\n",
      "run [450]:\n",
      "Style Loss : 1.890958 Content Loss: 4.800988\n",
      "\n",
      "run [500]:\n",
      "Style Loss : 1.626522 Content Loss: 4.688807\n",
      "\n",
      "run [550]:\n",
      "Style Loss : 1.435950 Content Loss: 4.599910\n",
      "\n",
      "run [600]:\n",
      "Style Loss : 1.298650 Content Loss: 4.520497\n",
      "\n",
      "run [650]:\n",
      "Style Loss : 1.195252 Content Loss: 4.451064\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m style_dataset \u001b[38;5;241m=\u001b[39m get_dataset_loader(style_folder, target_size)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Run style transfer on the dataset\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mrun_style_transfer_on_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn_normalization_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn_normalization_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mcontent_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m, in \u001b[0;36mrun_style_transfer_on_dataset\u001b[1;34m(cnn, normalization_mean, normalization_std, content_dataset, style_dataset, num_steps, style_weight, content_weight, output_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m input_img \u001b[38;5;241m=\u001b[39m content_img\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Run style transfer\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mrun_style_transfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalization_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalization_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcontent_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Save the output image\u001b[39;00m\n\u001b[0;32m     14\u001b[0m torchvision\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39msave_image(output, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[11], line 44\u001b[0m, in \u001b[0;36mrun_style_transfer\u001b[1;34m(cnn, normalization_mean, normalization_std, content_img, style_img, input_img, num_steps, style_weight, content_weight)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m style_score \u001b[38;5;241m+\u001b[39m content_score\n\u001b[1;32m---> 44\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# a last correction...\u001b[39;00m\n\u001b[0;32m     47\u001b[0m input_img\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lbfgs.py:438\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter \u001b[38;5;241m!=\u001b[39m max_iter:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;66;03m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;66;03m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;66;03m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 438\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    439\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[0;32m    440\u001b[0m     opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 33\u001b[0m, in \u001b[0;36mrun_style_transfer.<locals>.closure\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m content_score \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m content_weight\n\u001b[0;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m style_score \u001b[38;5;241m+\u001b[39m content_score\n\u001b[1;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m run[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define paths to your content and style folders\n",
    "    content_folder = \"C:/Users/AYUSH/Deep Learning Project/content/\"\n",
    "    style_folder = \"C:/Users/AYUSH/Deep Learning Project/style/\"\n",
    "\n",
    "\n",
    "    # Specify the target size for resizing images\n",
    "    target_size = (250, 350)\n",
    "\n",
    "    # Import VGG model\n",
    "    cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "    # Normalization parameters\n",
    "    cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "    cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "    # Get content and style datasets\n",
    "    content_dataset = get_dataset_loader(content_folder, target_size)\n",
    "    style_dataset = get_dataset_loader(style_folder, target_size)\n",
    "\n",
    "    # Run style transfer on the dataset\n",
    "    run_style_transfer_on_dataset(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                                  content_dataset, style_dataset, num_steps, style_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daacd60-2429-4b23-8df5-9f8bd77d22f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
